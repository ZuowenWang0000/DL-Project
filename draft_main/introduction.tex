\section{Introduction}\label{sec:intro}

In recent years, deep neural networks (DNNs) have become the tool of choice for image classification tasks. State-of-the-art DNN models are able to achieve very high accuracies on standard datasets. However, the complexity of these neural networks has resulted in human beings being unable to fully understand their training and inference process, causing difficulties in interpreting certain counterintuitive phenomena. One such phenomenon is the existence of \textit{adversarial examples} \cite{Biggio2013, Szegedy13}, images that appear to humans to be obvious examples of a certain class $y_{true}$ of objects (e.g. $y_{true}$~=~`cat'), but that the model assigns to a completely different class $y_{false}$ with very high confidence (e.g. $y_{false}$~=~`dog' with 99\% confidence). In some cases, even simple low-dimensional transformations such as translation or rotation can cause misclassification \cite{Engstrom17}. The existence of adversarial examples \cite{Goodfellow14,Madry18,Engstrom17}, as well as numerous defense schemes \cite{Zhang19, Wong18, Ragunathan18, Engstrom18, Kannan18}, has been well-studied. However, the mechanism underlying their existence is presently not known. 

Recent work by Ilyas et al. \cite{Ilyas2019} has put forth the hypothesis that the emergence of adversarial examples is due to the supervised learning paradigm currently employed, which solely aims to maximize categorical accuracy on natural (unperturbed) images. More specifically, they propose that each image class $y$ may have two main kinds of features associated with it: \textit{robust} features, features that are clearly indicative of class $y_{true}$ both to humans and to neural networks (e.g. the existence of fur or pointed ears), and \textit{non-robust} features, features that are strongly indicative of the class $y_{true}$, but are only meaningful to the neural network and not to humans, to whom they may appear to be random patterns. While non-robust features aid in boosting classification accuracy, they are \textit{brittle}, meaning that slight perturbations to the image (imperceptible to humans) can completely transform the non-robust features to indicate a different class $y_{false}$. The core idea is that such perturbations to non-robust features are the root cause of the existence of adversarial examples, and the dominant supervised learning paradigm causes neural networks to make heavy use of non-robust features to maximize natural accuracy. Ilyas et al. claim that this is the reason adversarial attacks tend to transfer well among diverse neural network architectures.

Ilyas et al. constructed two special datasets to verify their hypothesis: from a base dataset $D$ containing natural images, they created a dataset $D_R$ containing images whose non-robust features cannot be relied on to indicate their class (i.e., their only useful features are robust) and a dataset $D_{NR}$ containing images whose only useful features are non-robust. $D_R$ is produced by using an adversarially-trained classifier $f_\mathcal{A}$ to distort a randomly selected image $x'$ to the label $y$ of a different image $x$. The intuition is that since $f_\mathcal{A}$ is adversarially trained, it will only distort the robust features of $x'$ to the label $y$, leaving the non-robust features untouched and thus not relevant to the class $y$. $D_{NR}$ is produced by constructing adversarial examples (whose non-robust features, by definition, point to a class $y_{false} \neq y_{true}$, but whose robust features point to $y_{true}$) and relabeling them to class $y_{false}$.

More research is required to fully understand if Ilyas et al.'s hypothesis holds, and if so, how and to what it applies. This project aims to clarify and extend some aspects of Ilyas et al.'s work by asking the following questions:

\refstepcounter{qcounter}
\subsection*{Question \arabic{qcounter}.\quad \rm{\textit{Does the current supervised training paradigm induce undesired utilization of non-robust features?}}} \label{q1}

We first ask whether it is possible that the current goal of maximizing a classifier's accuracy during training can lead to the classifier ``overfitting'' to non-robust features, i.e. primarily making use of robust features in the early stages of training but heavily relying on non-robust features later to bring classification accuracy to its peak level. We design an experiment that snapshots training progress at regular intervals, performs various adversarial attacks, and then calculates the attack success rate ($ASR~=~\frac{\rm{number~of~adversarial~examples~wrongly~classified}}{\rm{number~of~corresponding~natural~examples~correctly~classified}}$). The intuition is that if neural networks first learn mainly robust features and only overfit to non-robust features in later training stages, the ASR should be lower in earlier stages of training.

\refstepcounter{qcounter}
\subsection*{Question \arabic{qcounter}.\quad \rm{\textit{Can we further validate the idea of natural images containing both robust and non-robust features?}}} \label{q2}

To answer this question, we design a dataset $D_{mix}$ containing (1) natural images of various classes from the original dataset $D$ and (2) images from $D_R$. We train various classifiers on this dataset and evaluate their robust accuracies for various different attacks as well as their natural accuracies. The intuition is that higher proportions of images from $D_R$ will lead to higher robust accuracies. Also, to eliminate the concern of defense information leakage to the constructed robust dataset \cite{engstrom2019a}, we evaluate the models trained on $D_{mix}$ with different attack methods. We expect that the robustness induced by $D_{mix}$ transfers well across different attack methods and architectures.

\refstepcounter{qcounter}
\subsection*{Question \arabic{qcounter}.\quad \rm{\textit{Can PGD-robust datasets induce spatial equivariance?}}} \label{q3}

Ilyas et al. only examine projected gradient descent (PGD) attacks \cite{Madry18} as a means of adversarial attack. We will therefore examine whether the ``robust features'' of \cite{Ilyas2019} are also robust against spatial attacks (such as translation and rotation). In particular, if such features exist, they should (by definition) induce a certain degree of equivariance to rotation and translation attacks for the model.

To test this hypothesis, we train classifiers on $D_R$ and perform spatial transformation attacks \cite{Engstrom17}. Since the adversarially-trained classifier $f_\mathcal{A}$ used to construct $D_R$ is only known to be robust to PGD attacks \cite{Madry18}, we also attempt to construct spatially robust and non-robust datasets $D_R^S$ and $D_{NR}^S$ using the same method as for $D_R$ and $D_{NR}$, but using special equivariance-inducing classifiers as $f_\mathcal{A}$. 

Finally, we train networks which claim to achieve a certain degree of spatial equivariance on $D_R$ to evaluate whether training only on robust features causes these networks to achieve a higher degree of spatial equivariance compared to training on natural images. The architectures we use include the group equivariant network \cite{Cohen16}, spatial transformer network \cite{Jaderberg15}, and a special case of equivariance transformer models \cite{Esteves18}, the polar transformer network \cite{Tai19}.

\textbf{Contributions.} Concretely, our contributions in this work are the following insights into the claims made in \cite{Ilyas2019}: 
\vspace{-7pt}
\begin{itemize}
    \itemsep-0.3em
    \item We show that the current training goal of maximizing accuracy on natural images is not in itself responsible for overfitting to non-robust features. In fact, we show that neural networks do not make a clear distinction between robust and non-robust features during training, utilizing both from the very beginning of training.
    \item Previous work \cite{Zhang19} has shown that there is often a trade-off between natural accuracy and robust accuracy. We show empirically that we can achieve a boost in robust accuracy by adding only a small amount of images from $D_R$ to the training dataset without perceptibly hurting natural accuracy. Furthermore, we show that the robustness of classifiers trained on $D_R$ transfers well among attacks based on different high-dimensional perturbations as well as among different architectures. We can therefore consider such mixed datasets as a novel defense mechanism against adversarial attacks.
    \item We attempt to apply the theories from \cite{Ilyas2019} to the case of spatial attacks, and find that their theories do not apply in the realm of low dimensional attacks. In particular, classifiers trained on $D_R$ exhibit worse spatial robustness than classifiers trained on $D$, even in the case of spatially robust architectures.
\end{itemize}