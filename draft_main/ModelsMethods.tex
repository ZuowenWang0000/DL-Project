\section{Models and Methods}

In this section, we go over the methods used to answer the Questions \ref{q1}, \ref{q2}, and \ref{q3}.

\subsection{Attacks} \label{sec:attacks}
In our work, we mainly study two major classes of attacks (i.e. image perturbations): PGD attacks and spatial 
attacks. The attack settings largely mirror those in \cite{Ilyas2019, engstrom2019a}. We mainly use these attacks for evaluation. The accuracy of a model evaluated with adversarial examples generated by an attack scheme $\mathcal{A}$ is called \textit{robust accuracy} under $\mathcal{A}$, while the original evaluation using only natural images is termed \textit{natural accuracy}. We use various attack parameters for the specific purpose of evaluating degrees of robustness for the different models trained. The character of a model which enables it to resist PGD or spatial attacks is called \textit{PGD robustness} or \textit{spatial robustness} respectively.

\paragraph{PGD attacks} Projected Gradient Descent (PGD) attacks are considered to be the strongest attacks which utilize the first-order information of the network, as studied in \cite{Madry18}. We use two types of PGD attacks (\ltwo-PGD and \linf-PGD) which adopt different norms when bounding the attack space: \ltwo-norm and \linf-norm. The adversarial examples are generated by perturbing the original images towards the gradient ascent direction, while the gradient is normalized by the constraint norm. The resulting perturbed image will be projected back to the allowed image space, with the norm of its difference from the original image constrained by a parameter $\epsilon$. This attack step can be performed for multiple iterations.

For the \ltwo-PGD attack, our attack parameters follow \cite{Ilyas2019}. We choose $\epsilon=0.25$ or $0.5$, step size $=0.1$ and we run 100 iterations to ensure that it is possible for the attack step to reach the boundary. For the \linf-PGD attack, apart from following the attack settings in \cite{Madry18}, we also set up experiments of smaller \linf boundary, i.e. weaker attacks, for our own purposes of demonstration. In total, we have $\epsilon = \frac{1}{255}, \frac{2}{255}, \frac{4}{255}$, and $\frac{8}{255}$ with step size $=\frac{\epsilon}{4}$ and 7 steps of attack. 

\paragraph{Spatial attacks} Spatial attacks are performed by generating rotated and/or translated images with the aim of causing misclassification. Previous work \cite{Engstrom17} has shown that a high number of spatial adversarial examples can be found through a \textit{grid search}. There are infinitely many combinations of rotation and translation possible for a spatial attack, so we attempt to approximate this infinite number by discretizing the continuous space of rotations and translations into a mesh containing all possible combinations of the discretized attack parameters. In particular, we use the following grids in our work. (1) \texttt{grid775}, a grid with 5 values per translation direction (vertical or horizontal) and 31 values for rotation, yielding 775 images; (2) \texttt{grid135}, a grid with 3 values per translation direction and 15 values for rotation, yielding 135 images (for these two grids, we use spatial limits of at most 3px translation in either direction and $30^\circ$ rotation in either direction); (3) \texttt{grid775,10$^\circ$}, the same as \texttt{grid775} but with at most $10^\circ$ of rotation in either direction; (4) \texttt{rot30}, the same as \texttt{grid775} but with no translation; and (5) \texttt{rot10}, the same as \texttt{rot30} but with at most $10^\circ$ of rotation in either direction. 

Attacks (2)--(5) serve as a weaker version of \texttt{grid775} to allow for the appropriate evaluation of the spatial robustness of certain models while avoiding being too strong to break any model claiming to achieve a certain degree of spatial robustness.

\subsection{Datasets}

The base dataset $D$ we use throughout this work is CIFAR-10 \cite{Krizhevsky09}. The datasets $D_R$ and $D_{NR}$ are derived from CIFAR-10 and are freely available as part of \cite{Ilyas2019}. Additionally, we create a custom dataset $D_{mix}$. This dataset is designed such that during training, a certain proportion $\alpha$ of each batch is composed of randomly selected images from $D_R$, while the rest are randomly selected from $D$. One epoch is defined to be over after 50000 images (the size of $D$).

\subsection{Architectures}

\paragraph{Main architectures} In this work we mainly conduct experiments with the ResNet architecture family \cite{He16}, namely ResNet-34 and ResNet-50. The VGG-16 \cite{Simonyan15} model and the ResNet-18 model are used for the purpose of studying the transferability of robustness which is induced by $D_R$ or $D_{mix}$, as the $D_R$ we used are constructed using ResNet-50 in \cite{Ilyas2019}. 

\paragraph{Spatial equivariant networks} To detect the effect of the PGD-robust dataset $D_R$ on special architecture designs aiming to gain equivariance against spatial transformations, we  compare the spatial robust accuracy from spatial equivariant networks trained with the natural dataset $D$ and the robust dataset $D_R$. Our scope of study includes: (a) G-ResNet18 and G-ResNet34 \cite{Cohen16} which use $p4m$ convolutional layers to replace normal 2D convolution layers in the ResNet-18 and ResNet-34 architectures, resulting in feature maps inherently containing rotations in 90$^\circ$ increments. We port the code from \cite{Bielski} in PyTorch into our experiment framework. (b) Spatial Transformer Networks (STNs) \cite{Jaderberg15}, which incorporate a pose predictor module in a backbone ResNet architecture. We adapt the standard STN code from the PyTorch STN tutorial \cite{pytorchstn}. (c) Polar Transformer Networks (PTNs) \cite{Esteves18}, an instantiation of Equivariant Transformer Networks \cite{Tai19} which exploit the rotational equivariance of convolution operations under polar coordinates. We adapt the code offered in \cite{Tai19}.

\subsection{Training details}

\paragraph{Hyperparameters} For experiments about Question \ref{q1} we use batch size 64, a weight decay of $2 \cdot 10^{-4}$, and an initial learning rate of 0.1 which is divided by 10 after 40000 and 60000 iterations for the ResNet-34 model. We train for 100 epochs in total. 

For experiments about Question \ref{q2} and \ref{q3} we use different hyperparameter settings than for Question \ref{q1}. In Question \ref{q2}, the ResNet-50, ResNet-18 and VGG-16 are trained using a batch size of 128, a weight decay of $5 \cdot 10^{-4}$, and an initial learning rate of 0.1 which is divided by 10 every 50 epochs. We train for 150 epochs in total. Due to constraints in computational resources, for Question \ref{q3} we tested two learning rates: 0.1 and 0.01 for the spatial equivariant networks.

\paragraph{Data augmentation} We use two variants of data augmentation. We call the first variant \texttt{std}, which performs a translation of at most 4px in any direction (chosen uniformly at random), performs a random horizontal flip with probability 0.5, jitters the brightness, contrast, and saturation by at most 25\% (chosen uniformly at random), and performs a rotation of at most $2^\circ$ in either direction (chosen uniformly at random). The second variant, \texttt{std*}, is the same as \texttt{std}, but performs a translation of at most $3\px$ in any direction and a rotation of up to $30^\circ$ in either direction.

\subsection{Methods}
For the three questions proposed in Section \ref{sec:intro} we designed a series of experiments to answer them. We describe the detailed methods in the following paragraphs.

\paragraph{Method for Question \ref{q1}} We use a TensorFlow \cite{Abadi2015} framework \cite{Engstrom17,Yang2019} to train a ResNet-34 and evaluate the ASRs for various attacks. In order to examine the claim proposed in \cite{Ilyas2019} that aiming to achieve a high categorical accuracy introduces adversarial vulnerabilities, we design experiments to detect the correlation of model natural accuracies and corresponding ASRs. We train a ResNet-34 model on the CIFAR-10 training set with \texttt{std} data augmentation for 100 epochs. To observe how ASRs for both PGD attacks and spatial attacks evolve with model natural accuracy, we evaluate them every 4 epochs during the training process of the model on the full test set of CIFAR-10. We also evaluate every 625 steps (equals 0.8 epochs) additionally before reaching the fourth epoch, in order to capture the rapid changes in natural accuracy and ASR during the beginning of training. The reason why we use ASR instead of robust accuracy as an evaluation metric is that it reflects how effective an attack method is and it rules out the influence of low natural accuracy of the model during the early training phase by ruling out examples which the model can not even make a correct prediction for in their natural state. 

\paragraph{Method for Question \ref{q2}} We use our framework with \texttt{std} data augmentation to train the ResNet-50, VGG-16, and ResNet-18 on $D_{mix}$ for values of $\alpha \in [0, 1]$ in 0.1-step increments and perform the following PGD attacks: (1) we attack all models with an \ltwo-norm attack constrained by $\epsilon = 0.25$ and 0.5 (with the learning rate and number of iterations described in Section \ref{sec:attacks}); (2) we attack the ResNet-50 with an \linf-norm attack constrained by $\epsilon = \frac{1}{255}, \frac{2}{255}, \frac{4}{255}$, and $\frac{8}{255}$ (with the step size and number of attack steps described in Section \ref{sec:attacks}).

\paragraph{Method for Question \ref{q3}} We use our framework to train the ResNet-50 and the G-ResNet, STN, and PTN (with as backbones ResNet-18 or ResNet-34). Each of the architectures is trained with \texttt{std} as well as \texttt{std*} data augmentation. We perform all the spatial attacks listed in Section \ref{sec:attacks} and log the best robust accuracies for each architecture by searching over the initial learning rates 0.1 and 0.01.

After careful consideration, we discovered that using the algorithm proposed in \cite{Ilyas2019} to construct $D_R$ cannot be used to construct a spatially robust dataset $D_R^S$ due to the fact that doing so would essentially be assigning different labels to the same object in different poses, which would not help with training. We also did not construct a spatially non-robust dataset $D_{NR}^S$ since after a careful grid search over several hyperparameters, we were unable to even achieve the results on $D_{NR}$ listed in \cite{Ilyas2019}.


Our code framework for studying Question \ref{q2} and Question \ref{q3} is built on the PyTorch \cite{Paszke19} \texttt{robustness} library \cite{robustness}.
% \subsection{Checking for ``overfitting'' to non-robust features} \label{sec:NRoverfitting}

% To evaluate whether neural networks typically ``overfit'' to non-robust features in the later stages of training, we train a TensorFlow \cite{Abadi2015} library implementing a ResNet-34 \cite{He16} on the CIFAR-10 dataset. We checkpoint the model every four epochs and then perform various adversarial attacks on the model, documenting the attack success rate (ASR) for each of them. We use two main types of attacks: a spatial grid search attack and an $L_{\infty}$-norm PGD attack. 

% We use three variations of the spatial grid search attack: \todo{ask}

% \subsection{Designing mixed datasets} \label{sec:mixedD}

% We extend the \texttt{robustness} library \cite{robustness} for this part. Specifically, we create a custom batch loader that makes sure that a certain proportion $\alpha$ of each batch is composed of images from $D_R$, while the rest are natural images from $D$ (here, $D$ is the CIFAR-10 dataset). The images from $D_R$ and $D$ are randomly selected, and one epoch is defined to be over after 50000 images (the size of $D$).

% We train a ResNet-50 \cite{He16} from the PyTorch \cite{Paszke19} standard library through the \texttt{robustness} library for various different values of $\alpha$, using a batch size of $128$, a weight decay of $5 \cdot 10^{-4}$, and an initial learning rate of $0.1$ which is divided by 10 every 50 epochs. After this, we evaluate the natural accuracy and the robust accuracy of each model under $L_2$ and $L_{\infty}$ attacks for various values of $\epsilon$ by using the evaluation suite from the \texttt{robustness} library.

% \subsection{Using $D_R$ for spatial attacks}

% We merge the spatial evaluation code from the spatial \texttt{robustness} library from \cite{Engstrom17} into our modified \texttt{robustness} library to perform spatial attacks on models trained on $D_R$. We evaluate a ResNet-50 trained with the same hyperparameters as in Section \ref{sec:mixedD} and log the robust accuracy obtained by the same grid search spatial attacks as in Section \ref{sec:NRoverfitting}.

% \subsection{Creating spatially robust datasets}

% \lipsum[1]

% \subsection{Training spatially equivariant classifers on $D_R$}

% \lipsum[1]