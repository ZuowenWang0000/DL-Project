\section{Conclusion}
In this work, we have studied three questions we raised from the hypothesis of the existence of robust and non-robust features in training data. We first restricted the scope of the study into a smaller set of attacks, by ruling out spatial attacks through monitoring the evolution of ASR with model natural accuracy. We found that the goal of maximizing accuracy is partially responsible for inducing PGD vulnerabilities, but a similar conclusion cannot be drawn for low-dimensional attacks. Furthermore, we found that mixing a certain amount of training examples from the PGD-robust dataset can boost the PGD-robustness by a great degree without substantially hurting natural accuracy, which can be considered as a defense mechanism embedded in the training dataset. This defense transfers well across various first-order attack methods and architectures. Finally, we investigated the relationship between the PGD-robust dataset and spatial equivariant networks and found that it does not help enhance spatial robustness. We speculate that there are no ``spatially robust features'' or ``spatially non-robust features''. This also justifies the no-trade-off phenomenon from \cite{Yang2019}. The spatial attack evaluation results, together with the conclusion for Question \ref{q1}, indicates that we should reexamine certain theories which are presumed to hold for both domains.